---
title: "Introduction"
---

<Frame>
  <img src="/img/trace.png" />
</Frame>

Traceloop's Python SDK allows you to easily start monitoring and debugging the execution of your LLM app.
Tracing is done in a non-intrusive way, built on top of OpenTelemetry.
You can choose to export the traces to Traceloop, or to your existing observability stack.

<Tip>
  You can use the SDK whether you use a framework like LangChain, or directly
  interact with a foundation model API.
</Tip>

```python
from traceloop.sdk import Traceloop
from traceloop.sdk.decorators import workflow

Traceloop.init(app_name="joke_generation_service")

@workflow(name="joke_creation")
def create_joke():
    completion = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": "Tell me a joke about opentelemetry"}],
    )

    return completion.choices[0].message.content
```

## Getting Started

Select from the following guides to learn more about how to use our Python SDK:

<CardGroup>
  <Card
    title="Getting Started"
    icon="bars-staggered"
    href="/python-sdk/getting-started"
  >
    Set up Traceloop Python SDK in your project
  </Card>
  <Card title="Decorators" icon="code" href="/python-sdk/decorators">
    Learn how to annotate your code to enrich your traces
  </Card>
  <Card title="Exporting" icon="share" href="/python-sdk/exporting">
    Learn how to connect to your existing observability stack
  </Card>
  <Card
    title="Prompt Management"
    icon="gear-complex-code"
    href="/python-sdk/prompts"
  >
    Manage your prompts and rollout changes with confidence
  </Card>
</CardGroup>
