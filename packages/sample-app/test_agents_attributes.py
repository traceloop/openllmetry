#!/usr/bin/env python3
"""
Test to capture and compare OpenAI agents instrumentation attributes
"""

import asyncio
import os
from dotenv import load_dotenv
from agents import Agent, function_tool, Runner
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import ConsoleSpanExporter
from opentelemetry.sdk.trace.export.in_memory_span_exporter import InMemorySpanExporter
from opentelemetry import trace
from traceloop.sdk import Traceloop
from traceloop.sdk.instruments import Instruments

# Load environment variables
load_dotenv()

# Set up in-memory span exporter to capture spans
memory_exporter = InMemorySpanExporter()
console_exporter = ConsoleSpanExporter()

# Initialize Traceloop with dashboard exporter
Traceloop.init(
    app_name="attribute-test", 
    disable_batch=True,
    instruments={Instruments.OPENAI, Instruments.OPENAI_AGENTS}
)

@function_tool
async def test_tool(input: str) -> str:
    """Test tool for attribute capture."""
    return f"Processed: {input}"

async def capture_attributes():
    """Capture all attributes from OpenAI agents instrumentation."""
    
    # Create a simple agent
    agent = Agent(
        name="Test Agent",
        instructions="You are a test agent. Use the test_tool when asked.",
        model="gpt-4o", 
        tools=[test_tool]
    )
    
    print("ğŸš€ Running test to capture attributes...")
    
    # Run a simple query
    query = "Please use the test tool with input 'hello world'"
    messages = [{"role": "user", "content": query}]
    
    runner = Runner().run_streamed(starting_agent=agent, input=messages)
    
    async for event in runner.stream_events():
        if event.type == "run_item_stream_event":
            if "tool" in event.name.lower():  
                print(f"ğŸ”§ {event.name}")
    
    print("âœ… Test complete!")
    
    print("\nâœ… Traces sent to Traceloop dashboard!")
    print("ğŸ”— Check your dashboard at: https://app.traceloop.com/")
    print("\nğŸ“Š Expected spans with complete OpenAI attributes:")
    print("1. ğŸŒ Agent Workflow (parent)")
    print("2. ğŸ¤– Test Agent.agent (child of workflow)")
    print("3. ğŸ“¡ openai.response (child of agent - LLM completion with ALL attributes)")
    print("4. ğŸ”§ test_tool.tool (child of agent)")
    
    print("\nğŸ” Key attributes to check in openai.response span:")
    print("âœ… gen_ai.system: 'openai'")
    print("âœ… gen_ai.request.model: 'gpt-4o-2024-08-06'")
    print("âœ… gen_ai.response.model: 'gpt-4o-2024-08-06'") 
    print("âœ… gen_ai.response.id: Complete response ID")
    print("âœ… gen_ai.usage.input_tokens: Token count")
    print("âœ… gen_ai.usage.output_tokens: Token count")
    print("âœ… gen_ai.usage.cache_read_input_tokens: Cache token count")
    print("âœ… llm.usage.total_tokens: Total token count")
    print("âœ… gen_ai.prompt.0.content: Full prompt text")
    print("âœ… gen_ai.prompt.0.role: 'user'")
    print("âœ… gen_ai.completion.0.role: 'assistant'")
    print("âœ… gen_ai.completion.0.tool_calls.0.*: Complete tool call data")
    print("âœ… llm.request.functions.0.name: Tool name")
    print("âœ… llm.request.functions.0.description: Tool description")
    print("âœ… llm.request.functions.0.parameters: Full JSON parameters")
    print("âœ… traceloop.span.kind: 'llm'")
    
    print(f"\nğŸ¯ Total attributes expected: 18+ (matching OpenAI responses instrumentation)")
    print("ğŸ“ˆ This demonstrates COMPLETE PARITY with official OpenAI instrumentation!")

if __name__ == "__main__":
    if not os.getenv("OPENAI_API_KEY"):
        print("âŒ Set OPENAI_API_KEY environment variable")
        exit(1)
    
    print("ğŸ¯ OpenAI Agents Attribute Comparison Test")
    print("=" * 50)
    asyncio.run(capture_attributes())