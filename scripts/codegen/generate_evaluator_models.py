#!/usr/bin/env python3
"""
Generate Pydantic models from OpenAPI/Swagger spec.
Extracts models used by v2/evaluators/execute/* endpoints.

Usage:
    python generate_evaluator_models.py <swagger_path> <output_dir>
"""

import json
import re
import subprocess
import sys
import tempfile
from pathlib import Path


def extract_definitions(swagger_path: str) -> dict:
    """Extract definitions used by v2/evaluators/execute/* endpoints."""
    with open(swagger_path) as f:
        data = json.load(f)

    all_definitions = data["definitions"]
    needed_refs = set()

    # Collect all definitions referenced by target endpoints
    for path, methods in data["paths"].items():
        if "/v2/evaluators/execute/" in path:
            for method, details in methods.items():
                # Get request body refs
                for param in details.get("parameters", []):
                    if "schema" in param and "$ref" in param["schema"]:
                        ref = param["schema"]["$ref"].replace("#/definitions/", "")
                        needed_refs.add(ref)
                # Get response refs
                for code, resp in details.get("responses", {}).items():
                    if "schema" in resp and "$ref" in resp["schema"]:
                        ref = resp["schema"]["$ref"].replace("#/definitions/", "")
                        needed_refs.add(ref)

    # Recursively find all referenced definitions
    def find_refs(obj, refs):
        if isinstance(obj, dict):
            if "$ref" in obj:
                ref = obj["$ref"].replace("#/definitions/", "")
                if ref not in refs:
                    refs.add(ref)
                    if ref in all_definitions:
                        find_refs(all_definitions[ref], refs)
            else:
                for v in obj.values():
                    find_refs(v, refs)
        elif isinstance(obj, list):
            for item in obj:
                find_refs(item, refs)

    # Find all nested references
    all_needed = set(needed_refs)
    for ref in list(needed_refs):
        if ref in all_definitions:
            find_refs(all_definitions[ref], all_needed)

    # Filter definitions to only include needed ones
    filtered_definitions = {
        k: v for k, v in all_definitions.items() if k in all_needed
    }

    return filtered_definitions


def generate_init_py(output_dir: Path) -> tuple[int, int]:
    """Generate __init__.py with proper exports."""
    # Extract class names from request.py
    request_classes = []
    request_file = output_dir / "request.py"
    if request_file.exists():
        content = request_file.read_text()
        request_classes = re.findall(r"^class (\w+)\(", content, re.MULTILINE)

    # Extract class names from response.py
    response_classes = []
    response_file = output_dir / "response.py"
    if response_file.exists():
        content = response_file.read_text()
        response_classes = re.findall(r"^class (\w+)\(", content, re.MULTILINE)

    # Generate __init__.py
    init_content = '''# generated by datamodel-codegen
# Models for v2/evaluators/execute endpoints from OpenAPI spec
#
# DO NOT EDIT MANUALLY - Regenerate with:
#   ./scripts/generate-models.sh /path/to/swagger.json

'''

    if request_classes:
        init_content += "from .request import (\n"
        for cls in sorted(request_classes):
            init_content += f"    {cls},\n"
        init_content += ")\n\n"

    if response_classes:
        init_content += "from .response import (\n"
        for cls in sorted(response_classes):
            init_content += f"    {cls},\n"
        init_content += ")\n\n"

    init_content += "__all__ = [\n"
    if request_classes:
        init_content += "    # Evaluator request models\n"
        for cls in sorted(request_classes):
            init_content += f'    "{cls}",\n'
    if response_classes:
        init_content += "    # Evaluator response models\n"
        for cls in sorted(response_classes):
            init_content += f'    "{cls}",\n'
    init_content += "]\n"

    (output_dir / "__init__.py").write_text(init_content)

    return len(request_classes), len(response_classes)


def main():
    if len(sys.argv) != 3:
        print(f"Usage: {sys.argv[0]} <swagger_path> <output_dir>")
        print(f"Example: {sys.argv[0]} /path/to/swagger.json ./generated")
        sys.exit(1)

    swagger_path = sys.argv[1]
    output_dir = Path(sys.argv[2])

    if not Path(swagger_path).exists():
        print(f"Error: Swagger file not found at {swagger_path}")
        sys.exit(1)

    print(f"=== Extracting definitions for evaluator execute endpoints ===")

    # Extract definitions
    filtered_definitions = extract_definitions(swagger_path)

    request_count = len(
        [k for k in filtered_definitions if k.startswith("request.")]
    )
    response_count = len(
        [k for k in filtered_definitions if k.startswith("response.")]
    )

    print(f"Extracted {len(filtered_definitions)} definitions")
    print(f"Request types: {request_count}")
    print(f"Response types: {response_count}")

    # Create JSON Schema with filtered definitions
    schema = {
        "$schema": "http://json-schema.org/draft-07/schema#",
        "definitions": filtered_definitions,
        "type": "object",
    }

    # Write to temp file
    with tempfile.NamedTemporaryFile(
        mode="w", suffix=".json", delete=False
    ) as f:
        json.dump(schema, f, indent=2)
        temp_schema = f.name

    print(f"=== Generating Pydantic models ===")

    # Create output directory
    output_dir.mkdir(parents=True, exist_ok=True)

    # Run datamodel-codegen
    try:
        subprocess.run(
            [
                "datamodel-codegen",
                "--input", temp_schema,
                "--input-file-type", "jsonschema",
                "--output", str(output_dir),
                "--output-model-type", "pydantic_v2.BaseModel",
                "--target-python-version", "3.10",
                "--use-standard-collections",
                "--reuse-model",
                "--disable-timestamp",
            ],
            check=True,
        )
    finally:
        # Cleanup temp file
        Path(temp_schema).unlink(missing_ok=True)

    print(f"=== Generating __init__.py with exports ===")

    req_count, resp_count = generate_init_py(output_dir)
    print(
        f"Generated __init__.py with {req_count} request "
        f"and {resp_count} response exports"
    )

    print(f"=== Model generation complete ===")
    print(f"Output written to: {output_dir}")


if __name__ == "__main__":
    main()
